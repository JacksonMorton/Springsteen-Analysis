{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_data = pd.read_csv('master_springsteen_data.csv')\n",
    "master_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "data = master_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary of words and their frequency for each album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics_list = map(lambda item: item.replace(\"'\", \"\").replace('\"','').replace(',,', ','), data.loc[:,'Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for song in range(0,len(lyrics_list)):\n",
    "    lyrics_list[song] = lyrics_list[song].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "lyrics_words = []\n",
    "album_words = []\n",
    "all_words = []\n",
    "\n",
    "for i in range(0,len(data)):   \n",
    "    if i > 0:\n",
    "        if data.loc[i,'Album'] != data.loc[(i-1),'Album']:\n",
    "            album_words.append(flatten(lyrics_words))\n",
    "            lyrics_words = [lyrics_list[i].replace(',,' , ',').split(',')]\n",
    "            all_words.append( lyrics_list[i].replace(',,' , ',').split(',') )\n",
    "        else: \n",
    "            lyrics_words.append( lyrics_list[i].replace(',,' , ',').split(',') )\n",
    "            all_words.append( lyrics_list[i].replace(',,' , ',').split(',') )\n",
    "    else: \n",
    "        lyrics_words.append( lyrics_list[i].replace(',,' , ',').split(',') ) \n",
    "        all_words.append( lyrics_list[i].replace(',,' , ',').split(',') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "album_word_dict = []\n",
    "\n",
    "for album in range(0,len(album_words)):\n",
    "    word_string = str(album_words[album]).replace(\"'\",\"\").replace('\\\\','').replace('.','').\\\n",
    "    replace('[','').replace(']','').replace('(','').replace(')','').replace('?','').replace(' ','')\n",
    "    word_string = word_string.split(',')\n",
    "    album_word_dict.append(dict(Counter(word_string))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of lists, where each sublist contains the words in each album, ordered from most to least frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_by_frequency = []\n",
    "for album in range(0,len(album_word_dict)):\n",
    "    sorted_list = list(reversed(sorted(album_word_dict[album], key=album_word_dict[album].get)))\n",
    "    words_by_frequency.append(sorted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary of every word in all songs, where the word is the key and its frequency is the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten(all_words)\n",
    "all_words_string = str(all_words).replace(\"'\",\"\").replace('\\\\','').replace('.','').\\\n",
    "replace('[','').replace(']','').replace('(','').replace(')','').replace('?','').replace(' ', '')\n",
    "all_words_string = all_words_string.split(',')\n",
    "all_words_dict = dict(Counter(all_words_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of lists, similar to 'words_by_frequency', but with words excluded if they appear more than an average of n times in all other albums. For example, if 'drive' is one of the most frequent words in 'Born to Run' but it appears, on average, n times or more in other albums, it would be removed from the list. This function can be tuned by adjusting the value of 'n'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words = [[] for i in range(len(words_by_frequency))]\n",
    "uniqueness_values = [[] for i in range(len(words_by_frequency))]\n",
    "\n",
    "for album in range(0,len(words_by_frequency)):\n",
    "    for word in range(0,len(words_by_frequency[album])):\n",
    "        uniqueness = (all_words_dict[words_by_frequency[album][word]] - album_word_dict[0][words_by_frequency[0][100]] ) / float(20)\n",
    "        uniqueness_values[album].append(uniqueness)\n",
    "        if uniqueness < 4:\n",
    "            unique_words[album].append(words_by_frequency[album][word])\n",
    "        else: pass      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a DataFrame where each row represents a unique album. The album names in this DataFrame will be used as the column names in the 'characteristic_words' DataFrame below.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Greetings from Asbury Park, N.J.</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wild, the Innocent, and the E Street Shuffle</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Born to Run</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Darkness on the Edge of Town</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The River</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Album  Song\n",
       "0                  Greetings from Asbury Park, N.J.     9\n",
       "1  The Wild, the Innocent, and the E Street Shuffle     7\n",
       "2                                       Born to Run     8\n",
       "3                      Darkness on the Edge of Town    10\n",
       "4                                         The River    20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albums = data[['Album','Song']].groupby('Album', as_index=False, sort=False).count()\n",
    "albums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a 'characteristic_words' DataFrame from the 'unique_words' list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characteristic_words = pd.DataFrame({albums.loc[0,'Album']:unique_words[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for album in range(1,len(album_word_dict)):\n",
    "    characteristic_words = characteristic_words.merge(pd.DataFrame({albums.loc[album,'Album']:unique_words[album]}), \\\n",
    "                                                      left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Greetings from Asbury Park, N.J.</th>\n",
       "      <th>The Wild, the Innocent, and the E Street Shuffle</th>\n",
       "      <th>Born to Run</th>\n",
       "      <th>Darkness on the Edge of Town</th>\n",
       "      <th>The River</th>\n",
       "      <th>Nebraska</th>\n",
       "      <th>Born in the U.S.A.</th>\n",
       "      <th>Tunnel of Love</th>\n",
       "      <th>Human Touch</th>\n",
       "      <th>Lucky Town</th>\n",
       "      <th>The Ghost of Tom Joad</th>\n",
       "      <th>18 Tracks</th>\n",
       "      <th>The Rising</th>\n",
       "      <th>Devils &amp; Dust</th>\n",
       "      <th>We Shall Overcome: The Seeger Sessions</th>\n",
       "      <th>Magic</th>\n",
       "      <th>Working on a Dream</th>\n",
       "      <th>The Promise</th>\n",
       "      <th>Wrecking Ball</th>\n",
       "      <th>High Hopes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>came</td>\n",
       "      <td>them</td>\n",
       "      <td>shes</td>\n",
       "      <td>prove</td>\n",
       "      <td>better</td>\n",
       "      <td>him</td>\n",
       "      <td>glory</td>\n",
       "      <td>alone</td>\n",
       "      <td>real</td>\n",
       "      <td>days</td>\n",
       "      <td>across</td>\n",
       "      <td>something</td>\n",
       "      <td>may</td>\n",
       "      <td>about</td>\n",
       "      <td>blowed</td>\n",
       "      <td>has</td>\n",
       "      <td>surprise</td>\n",
       "      <td>whoa</td>\n",
       "      <td>ground</td>\n",
       "      <td>raise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mary</td>\n",
       "      <td>johnny</td>\n",
       "      <td>backstreets</td>\n",
       "      <td>these</td>\n",
       "      <td>darlin</td>\n",
       "      <td>everything</td>\n",
       "      <td>days</td>\n",
       "      <td>honey</td>\n",
       "      <td>nothin</td>\n",
       "      <td>these</td>\n",
       "      <td>youngstown</td>\n",
       "      <td>dum</td>\n",
       "      <td>sky</td>\n",
       "      <td>thinkin</td>\n",
       "      <td>uh-huh</td>\n",
       "      <td>youll</td>\n",
       "      <td>youve</td>\n",
       "      <td>da</td>\n",
       "      <td>rocky</td>\n",
       "      <td>hopes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>could</td>\n",
       "      <td>boys</td>\n",
       "      <td>run</td>\n",
       "      <td>working</td>\n",
       "      <td>ooh</td>\n",
       "      <td>stop</td>\n",
       "      <td>usa</td>\n",
       "      <td>look</td>\n",
       "      <td>lovin</td>\n",
       "      <td>better</td>\n",
       "      <td>border</td>\n",
       "      <td>usa</td>\n",
       "      <td>empty</td>\n",
       "      <td>nothin</td>\n",
       "      <td>lord</td>\n",
       "      <td>alive</td>\n",
       "      <td>seen</td>\n",
       "      <td>sha</td>\n",
       "      <td>drawn</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>too</td>\n",
       "      <td>wont</td>\n",
       "      <td>hey</td>\n",
       "      <td>badlands</td>\n",
       "      <td>youve</td>\n",
       "      <td>put</td>\n",
       "      <td>born</td>\n",
       "      <td>two</td>\n",
       "      <td>mans</td>\n",
       "      <td>lucky</td>\n",
       "      <td>line</td>\n",
       "      <td>born</td>\n",
       "      <td>li</td>\n",
       "      <td>bed</td>\n",
       "      <td>hold</td>\n",
       "      <td>comin</td>\n",
       "      <td>working</td>\n",
       "      <td>breakaway</td>\n",
       "      <td>shackled</td>\n",
       "      <td>harry_s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sure</td>\n",
       "      <td>train</td>\n",
       "      <td>freeze-out</td>\n",
       "      <td>till</td>\n",
       "      <td>point</td>\n",
       "      <td>sir</td>\n",
       "      <td>hometown</td>\n",
       "      <td>gone</td>\n",
       "      <td>job</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>highway</td>\n",
       "      <td>thats</td>\n",
       "      <td>faith</td>\n",
       "      <td>god</td>\n",
       "      <td>pay</td>\n",
       "      <td>die</td>\n",
       "      <td>itself</td>\n",
       "      <td>darling</td>\n",
       "      <td>ball</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Greetings from Asbury Park, N.J.  \\\n",
       "0                             came   \n",
       "1                             mary   \n",
       "2                            could   \n",
       "3                              too   \n",
       "4                             sure   \n",
       "\n",
       "  The Wild, the Innocent, and the E Street Shuffle  Born to Run  \\\n",
       "0                                             them         shes   \n",
       "1                                           johnny  backstreets   \n",
       "2                                             boys          run   \n",
       "3                                             wont          hey   \n",
       "4                                            train   freeze-out   \n",
       "\n",
       "  Darkness on the Edge of Town The River    Nebraska Born in the U.S.A.  \\\n",
       "0                        prove    better         him              glory   \n",
       "1                        these    darlin  everything               days   \n",
       "2                      working       ooh        stop                usa   \n",
       "3                     badlands     youve         put               born   \n",
       "4                         till     point         sir           hometown   \n",
       "\n",
       "  Tunnel of Love Human Touch Lucky Town The Ghost of Tom Joad  18 Tracks  \\\n",
       "0          alone        real       days                across  something   \n",
       "1          honey      nothin      these            youngstown        dum   \n",
       "2           look       lovin     better                border        usa   \n",
       "3            two        mans      lucky                  line       born   \n",
       "4           gone         job  beautiful               highway      thats   \n",
       "\n",
       "  The Rising Devils & Dust We Shall Overcome: The Seeger Sessions  Magic  \\\n",
       "0        may         about                                 blowed    has   \n",
       "1        sky       thinkin                                 uh-huh  youll   \n",
       "2      empty        nothin                                   lord  alive   \n",
       "3         li           bed                                   hold  comin   \n",
       "4      faith           god                                    pay    die   \n",
       "\n",
       "  Working on a Dream The Promise Wrecking Ball High Hopes  \n",
       "0           surprise        whoa        ground      raise  \n",
       "1              youve          da         rocky      hopes  \n",
       "2               seen         sha         drawn       high  \n",
       "3            working   breakaway      shackled    harry_s  \n",
       "4             itself     darling          ball      would  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characteristic_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export 'characteristic_words' to a csv file. This file will be formatted and modified to include the top 10 rows in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characteristic_words.to_csv('characteristic_words.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
